{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading things\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function\n",
    "    \"\"\"\n",
    "    denominator = 1.0 + np.exp(-1.0 * X)\n",
    "    return 1.0 / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_gradient(a):\n",
    "    \"\"\"\n",
    "    computes the gradient of the sigmoid function at input value z\n",
    "    \"\"\"\n",
    "    a = np.array(a)\n",
    "    output_shape = a.shape\n",
    "    output = np.array([a * (1 - a)])\n",
    "    output.reshape(output_shape)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def square_up(theta_vector, input_layer_size, hidden_layer_size):\n",
    "    \"\"\"\n",
    "    Returns the theta vector to matrices\n",
    "    \"\"\"\n",
    "    vector_1_length = (input_layer_size + 1) * hidden_layer_size\n",
    "    matrix_1 = theta_vector[0:vector_1_length].reshape((hidden_layer_size, input_layer_size + 1))\n",
    "    matrix_2 = theta_vector[vector_1_length:]\n",
    "    matrix_2 = matrix_2.reshape((output_layer_size, hidden_layer_size + 1))\n",
    "    return (matrix_1, matrix_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten_out(thetas):\n",
    "    \"\"\"\n",
    "    Converts features from matrices to vectors.\n",
    "    \"\"\"\n",
    "    return np.hstack((thetas[0].reshape(-1), thetas[1].reshape(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_propogation(theta1, theta2, X):\n",
    "    \"\"\"\n",
    "    Forward propogation for a three layer nn\n",
    "    Inputs:\n",
    "        i = input layer\n",
    "        h = hidden layer\n",
    "        o = output layer (number of classes)\n",
    "        m = number of training sets / observations\n",
    "        theta1: (i+1) x h numpy array\n",
    "        theta2: (h+1) x o numpy array\n",
    "        X: m x i numpy array\n",
    "    \n",
    "    \"\"\"\n",
    "    # input layer\n",
    "    m, n = X.shape\n",
    "    a1 = np.ones((m, n+1))\n",
    "    a1[:,1:] = X\n",
    "    # hidden Layer\n",
    "    z2 = np.dot(theta1, a1.T)\n",
    "    a2_0 = sigmoid(z2)\n",
    "    a2 = np.ones((a2_0.shape[0]+1, m))\n",
    "    a2[1:,:] = a2_0\n",
    "    # output layer\n",
    "    z3 = np.dot(theta2, a2)\n",
    "    a3 = sigmoid(z3)    \n",
    "    return a1, z2, a2, z3, a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cost_function(thetas, m, X, Y, input_layer_size, hidden_layer_size, output_layer_size):\n",
    "    \"\"\"\n",
    "    Calculates the cost function J after a round of forward propogation\n",
    "    Inputs\n",
    "        i = input layer\n",
    "        h = hidden layer\n",
    "        o = output layer (number of classes)\n",
    "        m = number of training sets / observations\n",
    "        theta1: (i+1) x h numpy array\n",
    "        theta2: (h+1) x o numpy array\n",
    "        X: m x i numpy array\n",
    "    Output is a float\n",
    "    \"\"\"\n",
    "    t = square_up(thetas, input_layer_size, hidden_layer_size)\n",
    "    a1, z1, a2, z2, a3 = forward_propogation(t[0], t[1], X)\n",
    "    h = a3.T\n",
    "    h_r = h.reshape(-1)\n",
    "    y = np.eye(10)[Y-1][:,0,:]\n",
    "    y_r = y.reshape(-1)\n",
    "    j1 = np.dot(y_r, np.log(h_r))\n",
    "    j2 = np.dot((1-y_r), np.log(1 - h_r))\n",
    "    J = (-1./len(Y)) * (j1+j2)\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost_function_reg(thetas, lamda, m, X, Y, input_layer_size, hidden_layer_size, output_layer_size):\n",
    "    \"\"\"\n",
    "    Calculates the regularized cost function J after a round of forward propogation\n",
    "    Inputs\n",
    "        thetas are rolled feature spaces\n",
    "        lamda is a float\n",
    "        i = input layer\n",
    "        h = hidden layer\n",
    "        o = output layer (number of classes)\n",
    "        m = number of training sets / observations\n",
    "        theta1: (i+1) x h numpy array\n",
    "        theta2: (h+1) x o numpy array\n",
    "        X: m x i numpy array\n",
    "    Output is a float\n",
    "    \"\"\"\n",
    "    base = cost_function(thetas, X, Y, input_layer_size, hidden_layer_size, output_layer_size)\n",
    "    reg_0 = lamda / float(2*m)\n",
    "    reg_1 = sum([t**2 for t in thetas])\n",
    "    reg_term = reg_0 * reg_1\n",
    "    return base + reg_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost_gradient(thetas, m, X, Y, input_layer_size, hidden_layer_size, output_layer_size):\n",
    "    \"\"\"\n",
    "    Approximates the gradient vector of the NN via backpropagation\n",
    "    \"\"\"\n",
    "    init1_shape = (hidden_layer_size, input_layer_size+1)\n",
    "    init2_shape = (output_layer_size, hidden_layer_size+1)\n",
    "    delta_1 = np.zeros(init1_shape)\n",
    "    delta_2 = np.zeros(init2_shape)\n",
    "    count = 0\n",
    "    t = square_up(thetas, input_layer_size, hidden_layer_size)\n",
    "    y = np.eye(10)[Y-1][:,0,:]\n",
    "    ## back propagation\n",
    "    for x in X:\n",
    "        a1, z2, a2, z3, a3 = forward_propogation(t[0], t[1], x.reshape(1, input_layer_size))\n",
    "        # layer three\n",
    "        # layer three\n",
    "        y = np.zeros(10)\n",
    "        y[Y[count]-1]=1\n",
    "        delta_3_k = (a3.T - y)\n",
    "        delta_3_k = delta_3_k.T\n",
    "        # layer two\n",
    "        term1 = np.dot(t[1].T, delta_3_k) \n",
    "        term2 = sigmoid_gradient(a2)[0]\n",
    "        delta_2_k = term1 * term2\n",
    "        delta_2_k = delta_2_k[1:]\n",
    "        # calculating delta terms\n",
    "        term_2_ij = np.dot(delta_3_k, a2.T)\n",
    "        term_1_ij = np.dot(delta_2_k, a1)\n",
    "        delta_2 = delta_2 + term_2_ij\n",
    "        delta_1 = delta_1 + term_1_ij\n",
    "        count+=1\n",
    "\n",
    "    delta_1 = delta_1 / float(m)\n",
    "    delta_2 = delta_2 / float(m)\n",
    "    deltas = flatten_out((delta_1, delta_2))\n",
    "    return deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost_gradient_reg(thetas, lamda, m, X, Y, input_layer_size, hidden_layer_size, output_layer_size):\n",
    "    \"\"\"\n",
    "    Approximates the regularised gradient vector of the NN via backpropagation\n",
    "    \n",
    "    \"\"\"\n",
    "    grad = cost_gradient(thetas, m, X, Y, input_layer_size, hidden_layer_size, output_layer_size)\n",
    "    grad = square_up(grad, input_layer_size, hidden_layer_size)\n",
    "    term1 = lamda / m\n",
    "    thetas = square_up(thetas, input_layer_size, hidden_layer_size)\n",
    "    t1 = np.zeros(thetas[0].shape)\n",
    "    t1[:,1:] = thetas[0][:,1:]\n",
    "    t2 = np.zeros(thetas[1].shape)\n",
    "    t2[:,1:] = thetas[1][:,1:]\n",
    "    grad1 = grad[0] + (term1 * t1)\n",
    "    grad2 = grad[1] + (term1 * t2)\n",
    "    grad = flatten_out((grad1, grad2))\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numerical_gradient(theta1, theta2, lamda, m, X, Y, input_layer_size, hidden_layer_size, output_layer_size):\n",
    "    \"\"\"\n",
    "    Approximates the gradient of the cost function by perturbing \n",
    "    element ij of layer ell by eps\n",
    "    \"\"\"\n",
    "    thetas = flatten_out((theta1, theta2))\n",
    "    f = []\n",
    "    eps_g = 1e-4\n",
    "    for i in range(len(thetas)):\n",
    "        print \"Element\", i, \"of\", len(thetas)\n",
    "        theta_high = thetas\n",
    "        theta_low = thetas\n",
    "        theta_high[i] = theta_high[i] + eps_g\n",
    "        theta_low[i] = theta_low[i] - eps_g\n",
    "        j_high = cost_function_reg(theta_high, lamda, m, X, Y, input_layer_size, hidden_layer_size, output_layer_size)\n",
    "        j_low = cost_function_reg(theta_low, lamda, m, X, Y, input_layer_size, hidden_layer_size, output_layer_size)\n",
    "        f_theta_approx = (j_high - j_low) / (2 * eps_g)\n",
    "        f.append(f_theta_approx[0])\n",
    "    return np.array(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#f_numerical = numerical_gradient(theta1, theta2, lamda, m, X, Y, y_large, input_layer_size, hidden_layer_size, output_layer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10285,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:21: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "# loading features\n",
    "theta = loadmat('ex3weights.mat')\n",
    "theta1 = theta['Theta1']\n",
    "theta2 = theta['Theta2']\n",
    "# loading training set\n",
    "dta = loadmat('ex3data1.mat')\n",
    "X = dta['X']\n",
    "Y = dta['y']\n",
    "m, n = X.shape \n",
    "#Y[Y==10] = 0\n",
    "\n",
    "input_layer_size = X.shape[1]\n",
    "hidden_layer_size = theta1.shape[0]\n",
    "output_layer_size = theta2.shape[0]\n",
    "\n",
    "lamda = 1.\n",
    "eps_init = 0.12\n",
    "init1 = np.random.rand(hidden_layer_size, input_layer_size+1) * 2 * eps_init - eps_init\n",
    "init2 = np.random.rand(output_layer_size, hidden_layer_size+1) * 2 * eps_init - eps_init\n",
    "thetas_0 = flatten_out((init1, init2))\n",
    "thetas = flatten_out((theta1, theta2))\n",
    "print thetas.shape\n",
    "theta_opt = minimize(\n",
    "    cost_function, x0=thetas_0, \n",
    "    args=(m, X, Y, input_layer_size, hidden_layer_size, output_layer_size), \n",
    "    method=\"TNC\", jac=cost_gradient, \n",
    "    options={\"maxiter\":500, \"disp\":True}).x\n",
    "theta_opt = square_up(theta_opt, input_layer_size, hidden_layer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction(a, Y):\n",
    "    y = np.eye(10)[Y-1][:,0,:]\n",
    "    pr = np.argmax(a.T, axis=1).reshape(Y.shape)\n",
    "    accuracy = 100 * sum((pr==(Y-1))) / float(len(Y))\n",
    "    print 'The neural network correctly predicts %f percent of the cells' % accuracy\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The neural network correctly predicts 99.940000 percent of the cells\n"
     ]
    }
   ],
   "source": [
    "prediction(a3, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
